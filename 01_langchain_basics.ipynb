{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890b2f1-f31a-4a79-96b0-7e52bf8bdb12",
   "metadata": {},
   "source": [
    "# 1: Langchain basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a472f-34eb-4a0b-af32-57dab0e470e0",
   "metadata": {},
   "source": [
    "Make sure you are using the python binary in the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f308cd-878a-466d-b4d1-e40253bbbead",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cesargmx/llm-workshop/llm-workshop/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "519c0e7c-7af3-4d6c-98ad-74536751f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           ID              SIZE      MODIFIED     \n",
      "llama3.2:latest                a80c4f17acd5    2.0 GB    2 days ago      \n",
      "llama3.1:8b-instruct-q4_0      62757c860e01    4.7 GB    3 months ago    \n",
      "llama3.1:70b                   d729c66f84de    39 GB     3 months ago    \n",
      "llama3.1:latest                62757c860e01    4.7 GB    3 months ago    \n",
      "llama3-groq-tool-use:latest    36211dad2b15    4.7 GB    3 months ago    \n",
      "llama3:latest                  365c0bd3c000    4.7 GB    3 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ae306-8c6c-433f-b8a4-d76884a26ccb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Basic LLM Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0c96b3-a3d3-4620-be21-bcf56256b3bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great question for all the fans of Westeros!\n",
      "\n",
      "The author who wrote \"A Song of Ice and Fire\" is George R. R. Martin. This series of fantasy novels is his magnum opus, and it has become one of the most popular and critically acclaimed works in modern fantasy.\n",
      "\n",
      "George R. R. Martin published the first book in the series, \"A Game of Thrones\", in 1996. Since then, he has written five more books in the series:\n",
      "\n",
      "1. A Game of Thrones (1996)\n",
      "2. A Clash of Kings (1999)\n",
      "3. A Storm of Swords (2000)\n",
      "4. A Feast for Crows (2005)\n",
      "5. A Dance with Dragons (2011)\n",
      "\n",
      "There are two more planned books in the series, \"The Winds of Winter\" and \"A Dream of Spring\", but they have yet to be published.\n",
      "\n",
      "Martin's work has been adapted into the hit HBO television series \"Game of Thrones\", which was based on the first book and subsequent novels in the series.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=1)\n",
    "response = llm.invoke(\"who wrote A Song of Ice and Fire?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e9346-4a6f-4275-9da5-22483de686f1",
   "metadata": {},
   "source": [
    "## Part 1: Chains\n",
    "\n",
    "LLMs can be combined with other components, such as external data sources or other LLMs, to create more complex applications.\n",
    "\n",
    "A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, LLMs, or utils.\n",
    "\n",
    "You may find more information in the official documentation: https://python.langchain.com/v0.1/docs/expression_language/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018649e-abd8-4e8a-a89d-e140214a7341",
   "metadata": {},
   "source": [
    "**Goals:** \n",
    "* Review the difference between invoke and stream,\n",
    "* Review the text and structured text output, and\n",
    "* Understand the difference between string prompt and chat prompt composition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1e67d-b12f-4492-b2fd-dca7570266ce",
   "metadata": {},
   "source": [
    "First we instantiate the llama3 model. More about [ChatOllama in its Github repository](https://github.com/ollama/ollama#model-library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3413ffe7-1ea7-4a5a-9934-8eba7175cd89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",   \n",
    "    # You can experiment with the following parameters:\n",
    "    temperature=0,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121faef1-a9fa-4d9b-b626-5dd846d3e160",
   "metadata": {},
   "source": [
    "#### Example 1: Text Output, String Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813b8a6-e829-4c7d-a01f-dfa51b820992",
   "metadata": {},
   "source": [
    "Here we create a prompt from a string template, and the chain. Notice this line of the code, where we piece together these different components into a single chain using the LangChain Expression Language (LCEL):\n",
    "\n",
    "```\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "The `|` symbol is similar to a [unix pipe operator](https://en.wikipedia.org/wiki/Pipeline_%28Unix%29), which chains together the different components, feeding the output from one component as input into the next component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843a6a65-159a-448b-8a46-eaaf54eb6732",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"Write me the lyrics for a 30 seconds jingle about {product}\")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax (LCEL)\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898636c8-5989-43cd-82d7-f6f900458f36",
   "metadata": {},
   "source": [
    "When we execute the chain, we need to pass the parameters required by the prompt. Note that if we execute the chain with `invoke` all the text is presented after it was fully generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e974e257-0618-4144-aaf0-c7e248848210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a possible 30-second jingle for home insurance:\n",
      "\n",
      "(Upbeat, catchy tune)\n",
      "\"Protect your place, with a smile on your face\n",
      "Home insurance from us, will show you a safer space\n",
      "We've got coverage that's strong and true\n",
      "So your home and family are safe, through and through\n",
      "Call us today, and let's get it right\n",
      "Home insurance from [Company Name], shining bright!\"\n",
      "\n",
      "(Note: You can replace [Company Name] with the actual name of the company)\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"product\": \"home insurance\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37123b2-7843-4b59-85e6-6f58bc837531",
   "metadata": {},
   "source": [
    "Now execute the chain with `stream`. Notice how we can update the output after each token is generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c466e6bf-d202-48e1-815b-f5e742aca3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a catchy 30-second jingle for cat food:\n",
      "\n",
      "(Upbeat, energetic tune)\n",
      "\"Whiskers happy, purrs so sweet\n",
      "Meow's favorite treat to eat\n",
      "Tasty kibbles, crunchy too\n",
      "[Brand Name] cat food, made just for you!\n",
      "Purr-fectly delicious, every single day\n",
      "[Brand Name] cat food, the best way!\"\n",
      "\n",
      "(Outro music: 2-3 seconds of a short musical phrase that fades out)\n",
      "\n",
      "This jingle is designed to be catchy and easy to remember, with a simple melody that can stick in listeners' heads. The lyrics highlight the key benefits of the cat food (whiskers happy, purrs sweet) and emphasize its delicious taste. Feel free to modify or adjust as needed!"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"product\": \"cat food\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcd487-5286-46e5-94f4-7c680ffc8c17",
   "metadata": {},
   "source": [
    "#### Example 2: Structured Output, message based prompt\n",
    "\n",
    "A Structured output ensures we can use the output as input for other services/models. We can also create a prompt via messages. This is useful for chat based agents. \n",
    "\n",
    "First we  define the [output schema](https://json-schema.org/understanding-json-schema/reference/object?fbclid=IwZXh0bgNhZW0CMTEAAR1Tn1FU7M7ULeq4bBVOQTpxGZHs7y_JECH5WQpBB7edx-GGCabw0ouU8GQ_aem_kH8CeqwShi3l86Z2oSdpsA) and instantiate the model, specifying json as output format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3cff3d8-c48b-4d29-bb54-357fe222407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"integer\"},\n",
    "        \"favorite_food\": {\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\"],\n",
    "}\n",
    "\n",
    "output_schema_as_string = json.dumps(output_schema, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5c332d-86c9-4687-acc1-a8a7bec88474",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    format=\"json\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6184c8-d081-4bed-a79c-c92080029ac6",
   "metadata": {},
   "source": [
    "We create the prompt with a list of messages. Each message is associated with content, and an additional parameter called `role`. For example, a chat message can be associated with an AI assistant, a human or a system role. You can read more about the different type of messages [here](https://python.langchain.com/v0.1/docs/modules/model_io/chat/message_types/).\n",
    "\n",
    "This association is necessary to help with [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot). While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response.\n",
    "\n",
    "In this example, we don't provide examples to the llm, and  only use message based prompting to compare it with string prompting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6386996f-6c93-4aaf-93f8-51c51ef22601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "messages = [\n",
    "    (\"system\", \n",
    "         \"You are a helpful assistant that will extract information about a person and produce \"\n",
    "         \"an output using the following json schema: {json_schema}\"),\n",
    "    (\"human\", \"{person_info}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbb88fab-f1f7-44c0-a469-4a946f683ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Cesar', 'age': 37, 'favorite_food': 'sushi'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"json_schema\": output_schema_as_string,\n",
    "        \"person_info\": \"Cesar is 37 and loves sushi\"\n",
    "    })\n",
    "\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295925b-4b1e-4f59-b344-9f1a380b3d73",
   "metadata": {},
   "source": [
    "#### Example 3: Message Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb97d1c-8a7b-4a11-8e04-f3b8fb021db0",
   "metadata": {},
   "source": [
    "Additionally, LangChain provides different types of `MessagePromptTemplate`. The most commonly used are `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, which create an AI message, system message and human message respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ee12aac-e6f7-46c1-97e6-c1e03cea2254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "         \"You are a helpful assistant that will extract information about a person and produce \"\n",
    "         \"an output using the following json schema: {json_schema}\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{person_info}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "chain = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffdfa9-6caf-4cbd-b07b-3b06c01ee6fe",
   "metadata": {},
   "source": [
    "Notice how if we change the output parser, we get the same result, but the object is a string instead of a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87fd75ff-af0c-4895-afa1-423f9b44b493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Cesar\",\n",
      "  \"age\": 37,\n",
      "  \"favorite_food\": \"sushi\"\n",
      "}\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "chain_str_parser = prompt | llm | StrOutputParser()\n",
    "response = chain_str_parser.invoke(\n",
    "    {\n",
    "        \"json_schema\": output_schema_as_string,\n",
    "        \"person_info\": \"Cesar is 37 and loves sushi\"\n",
    "    })\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffafbd3-340b-4e51-9a6d-57a4f8bcce4f",
   "metadata": {},
   "source": [
    "The power from LLMs is that it can be used to extract information from natural language and produce an output in a format that can be used for integrations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58c79c15-b441-4e8b-8699-b92197048e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Laura', 'age': 30, 'favorite_food': 'decadent chocolate lava cake'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_info_text = \"\"\"As Laura walked into the cozy café, her bright smile illuminated the room, and \n",
    "her sparkling eyes seemed to dance with a youthful energy. Her long, curly brown hair bounced with \n",
    "each step, framing her heart-shaped face and emphasizing her petite nose ring. She was dressed in \n",
    "a flowy sundress, its vibrant floral pattern reflecting her playful personality. As she scanned the \n",
    "menu, her eyes widened with excitement, and she couldn't help but let out a little squeal of delight\n",
    "when she spotted her favorite dish: a decadent chocolate lava cake. She had always been a \n",
    "self-proclaimed chocoholic, and the mere mention of the rich, gooey treat was enough to transport \n",
    "her back to her childhood days of baking with her grandmother. With a spring in her step, she \n",
    "ordered her beloved dessert and settled in for a delightful afternoon of indulgence, to celebrate  \n",
    "her 30th birthday.\"\"\"\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"json_schema\": output_schema_as_string,\n",
    "        \"person_info\": person_info_text\n",
    "    })\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f0adeb-4026-468d-b69f-89e9c7eef2d5",
   "metadata": {},
   "source": [
    "A PE use case for this type of prompt is to extract information about HiVA Tasks, so they can be properly triaged. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a311b54-d3ce-4396-8713-e9a10dc2d61a",
   "metadata": {},
   "source": [
    "## Part 2: Chaining two prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f432d0f-70f0-4c1a-99c8-b8a56fa6d005",
   "metadata": {},
   "source": [
    "In this example we use the output of a LLM prompt as the input for another. There are two approaches, the first one is with the [LangChain Expression Language (LCEL)](https://python.langchain.com/v0.1/docs/expression_language/) and the second one with SimpleSequentialChain (This is obsolete and may be deprecated soon, adding it here for completeness)\n",
    "\n",
    "#### Example 4: LCEL Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8964887f-1ca7-491e-b71d-eadc734b0bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make Carpaccio di Manzo at home, follow these simple steps:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* Thinly sliced raw beef (such as ribeye or sirloin)\n",
      "* Fresh arugula\n",
      "* Shaved Parmesan cheese\n",
      "* Extra virgin olive oil\n",
      "* Lemon wedges\n",
      "* Black pepper\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. Place the thinly sliced beef on a plate.\n",
      "2. Top with fresh arugula leaves.\n",
      "3. Sprinkle shaved Parmesan cheese over the arugula.\n",
      "4. Drizzle extra virgin olive oil over the dish.\n",
      "5. Serve with lemon wedges and black pepper on the side.\n",
      "\n",
      "**Enjoy your Carpaccio di Manzo!**\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "template = \"\"\"Your job is to come up with a classic dish from the geographic area that the user suggests.\n",
    "% GEOGRAPHIC AREA: {user_input_area}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_location = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL: {link_output_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_meal = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt_location | llm | prompt_meal | llm | output_parser\n",
    "\n",
    "response = chain.invoke(\"Rome\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a7d05-afc5-4642-a423-a853c5bd0583",
   "metadata": {},
   "source": [
    "#### Example 5: Using SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cce1cf33-763b-4da0-a90e-961db0f40da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q2/cpqjhqrn1tzbgvfrg_97nnxh0000gn/T/ipykernel_58905/2959992215.py:19: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  location_chain = LLMChain(llm=llm, prompt=prompt_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mA culinary challenge!\n",
      "\n",
      "Given that our user is located in Rome, I'd like to suggest a classic Roman dish...\n",
      "\n",
      "**Carbonara**\n",
      "\n",
      "A rich and creamy pasta dish made with spaghetti, bacon or pancetta, eggs, parmesan cheese, and black pepper. The simplicity of its ingredients belies the depth of flavor it achieves, making Carbonara a beloved staple of Roman cuisine.\n",
      "\n",
      "How does that sound?\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mCarbonara is an excellent choice for Rome! Here's a simple recipe to make this classic dish at home:\n",
      "\n",
      "**Classic Roman Carbonara Recipe**\n",
      "\n",
      " Servings: 4\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 12 oz spaghetti\n",
      "* 6 slices of pancetta or bacon, diced\n",
      "* 3 large eggs\n",
      "* 1/2 cup grated Parmesan cheese\n",
      "* Black pepper, freshly ground\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Bring a large pot of salted water to a boil and cook the spaghetti according to package instructions until al dente.\n",
      "2. While the pasta cooks, cook the pancetta or bacon in a pan over medium heat until crispy. Remove from heat and set aside.\n",
      "3. In a medium bowl, whisk together the eggs, Parmesan cheese, and a pinch of black pepper.\n",
      "4. Reserve 1 cup of the cooked spaghetti water before draining the rest.\n",
      "5. Add the cooked spaghetti to the bowl with the egg mixture and toss well.\n",
      "6. Add the reserved pasta water to the bowl in small increments until the spaghetti is well coated with the creamy egg mixture.\n",
      "7. Finally, add the crispy pancetta or bacon to the bowl and toss everything together.\n",
      "\n",
      "Buon appetito!\n",
      "\n",
      "(Note: Traditionally, Carbonara does not contain any cream. The creaminess comes from the eggs and pasta water. If you're feeling adventurous, try making it without cream for a more authentic Roman experience!)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0.7,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION: {user_input_area}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_input_area\"], template=template)\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Holds the'location' chain\n",
    "location_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL: {link_output_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"link_output_meal\"], template=template)\n",
    "\n",
    "# Holds the'meal' chain\n",
    "meal_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)\n",
    "\n",
    "review = overall_chain.invoke(\"Rome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8b648-8c1c-45b6-ae1b-c41238c2bbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d21b5d6-04b6-4510-acef-b1f04097b0e4",
   "metadata": {},
   "source": [
    "### Part 3: LLama Prompt Template format "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aebcc2-a8de-4e57-a81e-240f8e2def26",
   "metadata": {},
   "source": [
    "In the previous example we used the following templates: \n",
    "\n",
    "```\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION: {user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "```\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL: {user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "You may notice how they may seem to follow certain format. Well, it turns out that there is an official [Llama 3's prompt format](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2). \n",
    "\n",
    "Following the official prompt format is a good idea for several reasons:\n",
    "1. **Consistency:** Using a standardized format ensures consistency across different prompts, making it easier to compare and analyze results.\n",
    "2. **Efficiency:** The official format is designed to be efficient, allowing you to provide all necessary information in a concise and organized manner.\n",
    "3. **Clarity:** The format helps to clarify the task and requirements, reducing the risk of misinterpretation or misunderstandings.\n",
    "4. **Reproducibility:** By following the same format, you can easily reproduce and build upon previous experiments, facilitating collaboration and research.\n",
    "5: **Optimization:** The official format is optimized for the LLaMA3 model, ensuring that the input is processed efficiently and effectively by the LLM.\n",
    "6. **Community alignment:** Using the official format aligns with the broader LLaMA3 community, making it easier to share and discuss results with others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51bfaf8-6f3e-4978-a923-67e97a7e9bd7",
   "metadata": {},
   "source": [
    "The template makes use of special tokens: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#-special-tokens-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a8f24-e417-4f92-8f37-a19b023231c2",
   "metadata": {},
   "source": [
    "#### Example 6: LLama template + Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aec5e2-8b5b-4e1e-9922-bee438524448",
   "metadata": {},
   "source": [
    "Pydantic is a popular Python library used for data validation, serialization, and deserialization. It provides a simple and intuitive way to define data models using Python type annotations.\n",
    "\n",
    "In our case, we will use it to get an object out of the LLM response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aecf6404-2e2b-4bca-a34f-9f912056f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic.v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic Schema for structured response\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\", required=True)\n",
    "    age: float = Field(description=\"The person's age\", required=True)\n",
    "    favorite_food: str = Field(description=\"The person's favorite food\", required=False)\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81a98f8a-a799-4207-8c19-5937dfe37bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The person\\'s name\", \"required\": true, \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"description\": \"The person\\'s age\", \"required\": true, \"type\": \"number\"}, \"favorite_food\": {\"title\": \"Favorite Food\", \"description\": \"The person\\'s favorite food\", \"required\": false, \"type\": \"string\"}}, \"required\": [\"name\", \"age\", \"favorite_food\"]}\\n```'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "105e10f6-bbb1-4e17-9a21-58f3a40e7b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a smart assistant. Take the following context and answer the following question.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "QUESTION: {question} \\n\n",
    "CONTEXT: {context} \\n\n",
    "JSON:\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbb0273f-a602-4a06-98cd-3536ca5c1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    #temperature=0.7,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de51b412-9244-4924-8d08-b6e8f361202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"It was a special day for Hermione, as she was celebrating her 17th birthday with her two best friends, Harry and Ron. As they sat down at the Three Broomsticks, Hermione couldn't help but feel grateful for her wonderful friends and another year of life. To celebrate, they ordered Hermione's favorite food, treacle tart.\"\"\"\n",
    "question = \"Tell me about Hermione\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82819d19-1e51-49a1-a4a9-3130feab1028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Hermione', 'age': 17, 'favorite_food': 'treacle tart'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\n",
    "    'context': context,\n",
    "    'question': question,\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c32e6f45-44b7-44fc-af59-688a99d51914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Hermione', age=17.0, favorite_food='treacle tart')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person(**result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91900f0b-707e-480e-9bce-44db0ea0047e",
   "metadata": {},
   "source": [
    "####  Question\n",
    "\n",
    "Try to run the same example, but use LLama 3.2. What do you observe? Why do you think that happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "746ed12e-be7c-432c-98d1-bd8701863960",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    #temperature=0.7,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d6e5020-2837-4e71-bb67-731d4dc0cbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': {'title': 'Hermione',\n",
       "  'description': \"The person's name\",\n",
       "  'required': True,\n",
       "  'type': 'string'},\n",
       " 'age': {'title': 'Age',\n",
       "  'description': \"The person's age\",\n",
       "  'required': True,\n",
       "  'type': 'number'},\n",
       " 'favorite_food': {'title': 'Favorite Food',\n",
       "  'description': \"The person's favorite food\",\n",
       "  'required': False,\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.invoke({\n",
    "    'context': context,\n",
    "    'question': question,\n",
    "})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f6792-4805-4326-a81b-f855a83a06e4",
   "metadata": {},
   "source": [
    "Why this is important? \n",
    "Langchain is a framework, and it is managing all the templating behind the scenes. It allows us to easily switch from one model to another, and the llm input will be changed accordingly. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
