{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e438bc0-511a-4c9b-86e1-33239bc0774c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 2: LLM Agents\n",
    "\n",
    "In this demo we have a basic LangChain agent. The objective is to get familiar with the architecture, make changes, and see how they affect the agent's behavior. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e6ddd-26f8-44cc-a507-91b538142afc",
   "metadata": {},
   "source": [
    "## Environment and ollama sanity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33512766-99e1-4614-bc06-e59f0e7c3048",
   "metadata": {},
   "source": [
    "Make sure you are using the python binary in the virtual environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775b6c5-d9a5-4c61-828e-540fdef69afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14380cf0-2f90-4088-8f0b-e68f88184f61",
   "metadata": {},
   "source": [
    "Ensure Ollama and the llama3 models are installed and properly setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad5ec9-eb1b-4a88-a6fc-651c7c640178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\", temperature=0.5)\n",
    "response = llm.invoke(\"who wrote A Song of Ice and Fire?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4224a1-1547-4c26-95be-dd547e93810c",
   "metadata": {},
   "source": [
    "## Part 1. Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c28390d-acc6-4ab0-928f-518a63688030",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The core idea of agents is to use a language model to choose a sequence of actions to take. These actions are defined as function or tool calls. \n",
    "\n",
    "### Chains vs Agents\n",
    "In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take (i.e. tools to run) and in which order.\n",
    "\n",
    "[Agent](https://www.internalfb.com/intern/px/p/5TD8R)\n",
    "\n",
    "<img src=\"Agent.png\" width=\"800\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b69670-2803-4e19-b107-6006f45cff82",
   "metadata": {},
   "source": [
    "### Agent Actions - Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af966635-9c98-4fea-8c16-813e2712850b",
   "metadata": {},
   "source": [
    "LLMs can benefit from using tools to overcome the limitations of their training data, such as outdated information and hallucinations. By providing LLMs with access to tools, they can answer questions within a controlled context that draws on existing knowledge bases and internal APIs. This allows LLMs to perform intermediate steps to gather relevant information and provide more accurate and up-to-date answers to questions. Additionally, tools can be used in combination to enhance the capabilities of LLMs. \n",
    "\n",
    "For example, a language model can be made to use a search tool to lookup quantitative information, perform analysis on the data, and then take actions based on the analysis. The first step in this example is to define the tools available to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9fe28-b935-46ca-a813-df8c29663656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"should_investor_buy_stock\",\n",
    "            \"description\":  \"This tool gets the investor id, and a stock symbol as input. It gets the \"\n",
    "                            \"investor profile and evaluates if it should buy stock from the provided symbol, \"\n",
    "                            \"according to the stock performance, the investor protfolio, their goals and \"\n",
    "                            \"risk profile. The tool returns a coefficient between (0, 1) representing how \"\n",
    "                            \"beneficial is for the investor to buy.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"symbol\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The stock symbol, e.g. META.\"\n",
    "                    },\n",
    "                    \"investor_id\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The ID of the investor.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"symbol\", \"investor_id\"]\n",
    "            }\n",
    "        },\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"should_investor_sell_stock\",\n",
    "            \"description\":  \"This tool gets the investor id, and a stock symbol as input. It gets the \"\n",
    "                            \"investor profile and evaluates if it should sell stock from the provided symbol, \"\n",
    "                            \"according to the stock performance, the investor protfolio, their goals and \"\n",
    "                            \"risk profile. The tool returns a coefficient between (0, 1) representing how \"\n",
    "                            \"beneficial is for the investor to sell.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"symbol\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The stock symbol, e.g. META.\"\n",
    "                    },\n",
    "                    \"investor_id\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The ID of the investor.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"symbol\", \"investor_id\"]\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "json.dumps(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15678bf-d2f6-4236-aa5e-e5d600605e64",
   "metadata": {},
   "source": [
    "Then we need to create a system prompt to give the LLM an instruction, let it be aware of the tools, and define a structured way to present the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d238b5-2daf-4835-a6cd-35f5a83d4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(\n",
    "        \"You are a quant assistant that helps investors answer questions about stock symbols. \"\n",
    "        \"Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt. \"\n",
    "        'Respond in the format [{{\"name\": function1_name, \"parameters\": dictionary of argument name and its value}},{{\"name\": function2_name, \"parameters\": dictionary of argument name and its value}},... ]. '\n",
    "        \"Do not use variables. NO other text MUST be included. \\n{tools_available}\"\n",
    "        \"Use the context bellow as extra information. \"\n",
    "        \"Context: \"\n",
    "        \"investor_id={investor_id}\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt.partial(\n",
    "    tools_available=json.dumps(tools).replace(\"{\", \"{{\").replace(\"}\", \"}}\"),\n",
    "    investor_id=239487\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec52ea-4e36-470c-98ed-0d9e75d8451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate \n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\", \n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bf713-bda8-4a9b-a9f7-8ab6c1339a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e98f32a-0577-42ff-b457-15a798af84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Question: Should I sell microsoft stock?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ffd5fe-229c-41e4-bf46-958de7d4275a",
   "metadata": {},
   "source": [
    "### Tool Calling\n",
    "\n",
    "Note that this is not executing the actual tools, but the LLM now knows based on the user input, if it should use a tool to expand its context to answer the question. Let's address this. \n",
    "\n",
    "First thing to note is that langchain offers tool support. This will make it easier to define and execute the tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04c5e0-9e11-4aaf-ba2b-7b4447a6c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def should_investor_buy_stock(symbol: str, investor_id: int) -> float:\n",
    "    \"\"\"This tool gets the investor id, and a stock symbol as input. It gets the\n",
    "    investor profile and evaluates if it should buy stock from the provided symbol,\n",
    "    according to the stock performance, the investor protfolio, their goals and\n",
    "    risk profile. The tool returns a coefficient between (0, 1) representing how \n",
    "    beneficial is for the investor to buy.\n",
    "    \"\"\"\n",
    "    return random.random()\n",
    "\n",
    "@tool\n",
    "def should_investor_sell_stock(symbol: str, investor_id: int) -> float:\n",
    "    \"\"\"This tool gets the investor id, and a stock symbol as input. It gets the\n",
    "    investor profile and evaluates if it should sell stock from the provided symbol,\n",
    "    according to the stock performance, the investor protfolio, their goals and\n",
    "    risk profile. The tool returns a coefficient between (0, 1) representing how \n",
    "    beneficial is for the investor to sell.\n",
    "    \"\"\"\n",
    "    return random.random()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d284ee76-245c-4879-9484-729b8fa2cfbe",
   "metadata": {},
   "source": [
    "You may have noticed that we added doc strings and a decorator to the function definition. The next cell shows the result of these changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522780f9-f263-4114-8162-d37d7dc420b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(should_investor_buy_stock.name)\n",
    "print(should_investor_buy_stock.description)\n",
    "print(should_investor_buy_stock.args)\n",
    "print(type(should_investor_buy_stock))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b357c-8180-447a-af3f-dca39cc6fd97",
   "metadata": {},
   "source": [
    "And we use the `invoke` method to execute the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810b723-6077-430c-8854-f6de60f2c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "should_investor_buy_stock.invoke(input={\"symbol\": \"META\", \"investor_id\": 123})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1915168-76f7-4d90-93aa-aff8f74d1866",
   "metadata": {},
   "source": [
    "Now we need to bind the tools to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d7248-4d36-4ce0-bbf8-241ea6f045d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ").bind_tools([should_investor_buy_stock, should_investor_sell_stock])\n",
    "llm.kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447a4854-039e-49f9-ba1d-991a9777b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"Question: Should I buy MSFT stock?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432016e7-5835-48c9-9276-8f2c87424975",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ce725-2d69-49f6-bccc-4e0bcbdecc9f",
   "metadata": {},
   "source": [
    "### E2E Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68419699-22ff-4152-aa32-fd4324a34f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create this variable to make calling functions easier\n",
    "tools_available = {\n",
    "    'should_investor_buy_stock': should_investor_buy_stock,\n",
    "    'should_investor_sell_stock': should_investor_sell_stock\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a44e9d-c0fe-4b82-afd8-90cab0c69628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "# We get the query from a UI tool.\n",
    "query = \"Should I buy MSFT stock?\"\n",
    "\n",
    "# Pass it to the conversation log\n",
    "messages = [HumanMessage(query)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032af62-169f-4dff-a82c-62da8abc9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the llm, and add the llm response to the conversation log\n",
    "ai_msg = llm.invoke(messages)\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c927d8-89cd-4813-954b-f32eb7f1c742",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc6d0c-e63f-4493-9421-edb910a3eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call = ai_msg.tool_calls[0]\n",
    "selected_tool = tools_available.get(tool_call['name'], None)\n",
    "selected_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a090d67-fba9-461b-a37f-6e0383ecde0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = tools_available.get(tool_call['name'], None)\n",
    "    if selected_tool:\n",
    "        tool_msg = selected_tool.invoke(tool_call)\n",
    "        messages.append(tool_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b7689-7777-4b1b-8375-d1e0a327ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156acdf-63f9-479a-89b0-2650bb639c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we invoke the model with the tool results\n",
    "llm.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c2f17-4156-40df-90e2-222061168407",
   "metadata": {},
   "source": [
    "### Exercise: Can you wrap the E2E flow in a single class/function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366aee19-c052-4dc6-8137-4d281f15266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3555640-96dc-4023-b2d1-2a848e7c3f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85f182-d2a5-4d42-81b2-93e210186631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e38bd-7d01-44b2-8d6a-e2e1162a07b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7854b-ecbb-4ef7-91d1-e53d77fa463f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9c5f31-5b6b-47bc-82d6-0777a16faedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52370c6-4c58-4615-8cd2-3ff1059536bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea4593b8-9576-4d77-a4cd-b7b14987d307",
   "metadata": {},
   "source": [
    "## Part 2: Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431a9b0-6ef9-4ba7-9605-5fa3282b35fc",
   "metadata": {},
   "source": [
    "The example we did in part 1 is very simple, it only has two tools. The real power from LLM agents comes from extending this basic flows to create graphs to answer queries and solve tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62e74c-4a32-4710-9bb7-1fd6d5473ecf",
   "metadata": {},
   "source": [
    "### Basic Chatbot Agent with Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de3112-04cf-4f9b-ba8f-c6021c436d2a",
   "metadata": {},
   "source": [
    "The first thing you do when you define a graph is define the `State` of the graph. The State consists of the schema of the graph as well as [reducer functions](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers) which specify how to apply updates to the state. In our example `State` is a `TypedDict` with a single key: `messages`. The `messages` key is annotated with the [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) reducer function, which tells LangGraph to append new messages to the existing list, rather than overwriting it. State keys without an annotation will be overwritten by each update, storing the most recent value. Check out this [conceptual guide](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages) to learn more about state, reducers and other low-level concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a5c5c-5028-4a3e-babe-d48b08022bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0074e185-a544-4ece-b912-fc91584b66dc",
   "metadata": {},
   "source": [
    "So now our graph knows two things:\n",
    "\n",
    "1. Every node we define will receive the current `State` as input and return a value that updates that state.\n",
    "2. `messages` will be appended to the current list, rather than directly overwritten. This is communicated via the prebuilt `add_messages` function in the Annotated syntax.\n",
    "\n",
    "   \n",
    "Next, add a \"chatbot\" node. Nodes represent units of work. They are typically regular python functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0953b3d-ec91-4b63-bab9-cec78c5a7160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Notice how the chatbot node function takes the current State as input and returns a dictionary \n",
    "# containing an updated messages list under the key \"messages\". This is the basic pattern for all \n",
    "# LangGraph node functions.\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed0c2c-b2ab-40ea-b156-acb2c4c0712d",
   "metadata": {},
   "source": [
    "Next, add an entry point. This tells our graph where to start its work each time we run it. Similarly, set a finish point. This instructs the graph \"any time this node is run, you can exit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b467b3a-03e8-498b-aab9-bee051b6563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759296a-61a5-4f7a-aa7f-6d5001dcb135",
   "metadata": {},
   "source": [
    "Finally, we'll want to be able to run our graph. To do so, call \"compile()\" on the graph builder. This creates a \"CompiledGraph\" we can use invoke on our state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aff3eb-83cd-4bee-acc5-640eaa546c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e30b5-eb74-40a7-b8e3-58405446b3d1",
   "metadata": {},
   "source": [
    "You can visualize the graph using the `get_graph` method and one of the \"draw\" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b7ca0-e969-4e70-b2d0-392940c426bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e8193-c7f5-4f91-b3a9-61ee024a8ba8",
   "metadata": {},
   "source": [
    "Now let's run the chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a32f86-dff4-4bc1-b578-1cb8744a8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        # exit the chat loop at any time by typing \"quit\", \"exit\", or \"q\"\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What is the symbol ticker for Meta?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff128d0-daf8-4ef3-b175-0bad911c9217",
   "metadata": {},
   "source": [
    "### Chatbot with tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cf493d-6472-4b22-bf67-f73397f6656f",
   "metadata": {},
   "source": [
    "We start with the same code as in the previous example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fffbf-2ba8-49e6-b673-bdb282eb4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools from part 1\n",
    "tools = [should_investor_buy_stock, should_investor_sell_stock]\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# We bind tools this time\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ").bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b14d33-13aa-4752-b973-b14487803788",
   "metadata": {},
   "source": [
    "Next we need to create a function to actually run the tools if they are called. We'll do this by adding the tools to a new node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8f487-5219-4802-a330-81cf87e041e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab0a32-f4e4-4688-85dc-7867bdac5a7e",
   "metadata": {},
   "source": [
    "With the tool node added, we can define the `conditional_edges`.\n",
    "\n",
    "Recall that **edges** route the control flow from one node to the next. Conditional edges usually contain \"if\" statements to route to different nodes depending on the current graph state. These functions receive the current graph state and return a string or list of strings indicating which node(s) to call next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c415cb3f-cf40-4f42-952d-77e454f33cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    ")\n",
    "\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# Starting point is the chatbot\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# Because the conditional edge can return END, we don't need to explivitly set a finish point\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b998c-ee74-413a-a905-7d30c29e846b",
   "metadata": {},
   "source": [
    "Let's visualize the graph we've built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b5a8a-9f27-4143-8d80-aa68ec47f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42310272-7772-41cf-bd36-61a59b6e5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What is the symbol ticker for Meta?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec20e22-e827-4a02-a5a7-bd636623d6da",
   "metadata": {},
   "source": [
    "You can use LangGraph for various applications such as:\n",
    "* Complex task automation, i.e. run books\n",
    "* Custom LLM-backed experiences, i.e. assistant that helps create incident reports from SEV chats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ac0b2-b923-4dfa-9639-d629e9921b60",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "There are pre-buillt components that make using LangGraph easier. Check them out in the official [API reference](https://langchain-ai.github.io/langgraph/reference/prebuilt/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
